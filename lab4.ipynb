{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25431b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df504f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.vocab = set()\n",
    "        self.total_ngrams = 0\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Convierte texto en una lista de palabras con tokens de inicio/fin.\"\"\"\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())  # limpia puntuación y pasa a minúsculas\n",
    "        tokens = text.split()\n",
    "        tokens = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
    "        return tokens\n",
    "\n",
    "    def _generate_ngrams(self, tokens: List[str]) -> List:\n",
    "        \"\"\"Genera pares (contexto, palabra objetivo)\"\"\"\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            context = tuple(tokens[i:i + self.n - 1])\n",
    "            target = tokens[i + self.n - 1]\n",
    "            ngrams.append((context, target))\n",
    "        return ngrams\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Entrena el modelo con una lista de párrafos o frases.\"\"\"\n",
    "        for text in texts:\n",
    "            tokens = self._tokenize(text)\n",
    "            ngrams = self._generate_ngrams(tokens)\n",
    "            for context, target in ngrams:\n",
    "                self.ngram_counts[context][target] += 1\n",
    "                self.vocab.add(target)\n",
    "                self.total_ngrams += 1\n",
    "\n",
    "        # Cálculo para Good-Turing\n",
    "        self.count_of_counts = Counter()\n",
    "        for context in self.ngram_counts:\n",
    "            for word in self.ngram_counts[context]:\n",
    "                count = self.ngram_counts[context][word]\n",
    "                self.count_of_counts[count] += 1\n",
    "\n",
    "    def predict_next(self, context: Tuple[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Predice la próxima palabra más probable dado un contexto.\"\"\"\n",
    "        if len(context) != self.n - 1:\n",
    "            raise ValueError(f\"El contexto debe tener longitud {self.n - 1}\")\n",
    "\n",
    "        target_counts = self.ngram_counts.get(context, {})\n",
    "        total = sum(target_counts.values())\n",
    "\n",
    "        if total == 0:\n",
    "            return []\n",
    "\n",
    "        return sorted(\n",
    "            [(word, count / total) for word, count in target_counts.items()],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "    \n",
    "    def good_turing_probability(self, context: Tuple[str, ...], word: str) -> float:\n",
    "        r = self.ngram_counts[context][word]\n",
    "        Nr = self.count_of_counts[r]\n",
    "        Nr_plus_1 = self.count_of_counts.get(r + 1, 0)\n",
    "\n",
    "        # Good-Turing smoothing only applies when we have observed data\n",
    "        if Nr > 0 and Nr_plus_1 > 0:\n",
    "            r_star = (r + 1) * (Nr_plus_1 / Nr)\n",
    "        else:\n",
    "            r_star = 1e-10  # Asignamos una probabilidad muy baja si no hay datos\n",
    "\n",
    "        total_context = sum(self.ngram_counts[context].values())\n",
    "        normalizer = total_context if total_context > 0 else 1\n",
    "\n",
    "        return r_star / normalizer\n",
    "\n",
    "    def probability(self, context: Tuple[str, ...], word: str, k: float = 1.0, smoothing: str = \"laplace\") -> float:\n",
    "        \"\"\"Devuelve la probabilidad de una palabra dado el contexto, usando Laplace o Good-Turing.\"\"\"\n",
    "        if len(context) != self.n - 1:\n",
    "            raise ValueError(f\"El contexto debe tener longitud {self.n - 1}\")\n",
    "\n",
    "        if smoothing == \"laplace\":\n",
    "            V = len(self.vocab)\n",
    "            count = self.ngram_counts[context][word] + k\n",
    "            total = sum(self.ngram_counts[context].values()) + k * V\n",
    "            return count / total\n",
    "        elif smoothing == \"good_turing\":\n",
    "            return self.good_turing_probability(context, word)\n",
    "        else:\n",
    "            raise ValueError(\"Smoothing no reconocido. Usa 'laplace' o 'good_turing'.\")\n",
    "    \n",
    "    def entropy(self, sentence: str, k: float = 1.0, smoothing: str=\"laplace\") -> Tuple[float, float]:\n",
    "        tokens = self._tokenize(sentence)\n",
    "        ngrams = self._generate_ngrams(tokens)\n",
    "\n",
    "        entropy = 0\n",
    "        for context, word in ngrams:\n",
    "            prob = self.probability(context, word, k=k, smoothing=smoothing)\n",
    "            entropy += -math.log2(prob)\n",
    "        \n",
    "        entropy_per_word = entropy / len(ngrams)\n",
    "        perplexity = 2 ** entropy_per_word\n",
    "        \n",
    "        return entropy_per_word, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54cdd652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Creamos un modelo de bigramas\n",
    "modelo = NGramModel(n=2)\n",
    "\n",
    "# Entrenamos con texto\n",
    "corpus = [    \n",
    "    \"El procesamiento de lenguaje natural es una rama de la inteligencia artificial que permite a las máquinas comprender y generar lenguaje humano. Esta tecnología se encuentra en aplicaciones cotidianas como asistentes virtuales, traductores automáticos y sistemas de recomendación. Su objetivo es reducir la brecha entre la forma en que las personas se comunican y cómo las computadoras procesan información.\",\n",
    "    \"Los modelos estadísticos como los N-gramas son fundamentales para tareas de PLN. Un modelo N-gram aprende las secuencias de palabras más comunes en un texto y asigna probabilidades a las siguientes palabras, basado en el contexto. Aunque es un enfoque simple comparado con modelos modernos, sigue siendo útil por su facilidad de implementación y análisis.\",\n",
    "    \"A pesar de sus ventajas, los N-gramas enfrentan el problema de datos escasos, ya que es difícil cubrir todas las combinaciones posibles de palabras. Para solucionar esto, se aplican técnicas como el smoothing, que ajustan las probabilidades para evitar ceros. Esto mejora la robustez del modelo al trabajar con nuevos textos o frases no vistas durante el entrenamiento.\"\n",
    "    ]\n",
    "modelo.train(corpus)\n",
    "\n",
    "contexto = (\"encuentra\",)\n",
    "print(modelo.predict_next(contexto)) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00821c",
   "metadata": {},
   "source": [
    "Si la palabra es \"encuentra\" la palabra más probable después es \"en\".\n",
    "\n",
    "Esto con una probabilidad de 1 dentro de lo que sabe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a41220",
   "metadata": {},
   "source": [
    "## Probabilidad de una palabra específica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fe9ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01694915254237288\n"
     ]
    }
   ],
   "source": [
    "# Obtener probabilidad específica\n",
    "print(modelo.probability((\"encuentra\",), \"en\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0154f",
   "metadata": {},
   "source": [
    "## Probabilidad con Smoothing Laplaciano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "028464de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00847457627118644\n"
     ]
    }
   ],
   "source": [
    "# Obtener probabilidad específica\n",
    "print(modelo.probability((\"encuentra\",), \"dulce\", smoothing=\"laplace\", k=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c9904",
   "metadata": {},
   "source": [
    "## Probabilidad con Smoothing Good-Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ee0bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-10\n"
     ]
    }
   ],
   "source": [
    "# Obtener probabilidad específica\n",
    "print(modelo.probability((\"encuentra\",), \"dulce\", smoothing=\"good_turing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416086df",
   "metadata": {},
   "source": [
    "La probabilidad usando smoothing  de que \"en\" esta después de \"encuentra\" es de 0.017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07b90b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia: 6.568271790961251\n",
      "Perplejidad: 94.89576446460957\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"se esta probando un ngramas con el problema de datos escasos\"\n",
    "H, PP = modelo.entropy(test_sentence)\n",
    "print(f\"Entropia: {H}\")\n",
    "print(f\"Perplejidad: {PP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf4a1f",
   "metadata": {},
   "source": [
    "Entropía = 6.568: en promedio el modelo necesita mucha información (6.57 bits) para adivinar bien la siguiente palabra.\n",
    "\n",
    "Perplejidad = 94.896: en promedio hay 94 palabras posibles para cada contexto, esto es muy alto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8432741",
   "metadata": {},
   "source": [
    "Entonces por la alta entropía (alta incertidumbre)  y también la alta perpelejidad (alta indecisión) podemos decir que por ahora el modelo no es muy bueno. Esto es debido a que por ahora se ha entrenado con un corpus muy limitado. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc410a8",
   "metadata": {},
   "source": [
    "No obstante, gracias al smoothing el modelo no colapsa ante combinaciones que tienen poca o nula frecuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57eeab1",
   "metadata": {},
   "source": [
    "# ACTIVIDAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cdf2a",
   "metadata": {},
   "source": [
    "1. el procesamiento de lenguaje\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37bfb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('natural', 0.5), ('humano', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "contexto = (\"lenguaje\",)\n",
    "print(modelo.predict_next(contexto)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e9739",
   "metadata": {},
   "source": [
    "2. el problema de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "812d807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('escasos', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "contexto = (\"datos\",)\n",
    "print(modelo.predict_next(contexto)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bf100",
   "metadata": {},
   "source": [
    "3. \tlos modelos estadísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a6012c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('como', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "contexto = (\"estadísticos\",)\n",
    "print(modelo.predict_next(contexto)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883c3bc",
   "metadata": {},
   "source": [
    "4. \tla inteligencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14023f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('artificial', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "contexto = (\"inteligencia\",)\n",
    "print(modelo.predict_next(contexto)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527e805",
   "metadata": {},
   "source": [
    "5. los ngramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "debe7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('son', 0.5), ('enfrentan', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "contexto = (\"ngramas\",)\n",
    "print(modelo.predict_next(contexto)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
